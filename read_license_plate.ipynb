{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pytesseract\n",
    "import imutils\n",
    "import easyocr\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "reader = easyocr.Reader(['de'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained yolo model\n",
    "lp_detect_model = YOLO('yolo_model/best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop the license plate and save as image\n",
    "def crop_license_plates(img, yolo_model, save_img=True):\n",
    "    img_name = img.split(\"\\\\\")[-1]\n",
    "    \n",
    "    \n",
    "    print(f\"Processing {img_name}\")\n",
    "    detected_lps = yolo_model.predict(img, verbose=False)[0]\n",
    "    license_plates = []\n",
    "    \n",
    "    for lp in detected_lps.boxes.data.tolist():\n",
    "        \n",
    "        x1, y1, x2, y2, conf, _ = lp\n",
    "       \n",
    "        img_array = cv2.imread(img)\n",
    "        cropped_lp =img_array[int(y1):int(y2), int(x1):int(x2), :]\n",
    "        license_plates.append(cropped_lp)\n",
    "        if save_img:\n",
    "            cv2.imwrite(f\"cropped_license_plates/cropped_{img_name}\", cropped_lp)\n",
    "        \n",
    "        \n",
    "    return license_plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr(image):\n",
    "    custom_config = r'--psm 6 --oem 3 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZÖ0123456789'\n",
    "    text = pytesseract.image_to_string(image, lang='deu', config=custom_config)\n",
    "    if text:\n",
    "        return text.strip(\"\\n\")\n",
    "    return \"not detected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_w_h(rect):\n",
    "    (tl, tr, br, bl) = rect\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    return maxWidth, maxHeight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_image(img, contour):\n",
    "    hull = cv2.convexHull(contour)\n",
    "\n",
    "    # Get the minimum area rectangle from the convex hull\n",
    "    rect = cv2.minAreaRect(hull)\n",
    "    \"\"\"\n",
    "    approx = cv2.approxPolyDP(contour, 0.015 * cv2.arcLength(contour, True), True)\n",
    "   \n",
    "    \n",
    "  \n",
    "    print(approx.shape)\n",
    "    points = approx.reshape(4, 2)\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    s = points.sum(axis = 1)\n",
    "    rect[0] = points[np.argmin(s)]\n",
    "    rect[2] = points[np.argmax(s)]\n",
    "\n",
    "    diff = np.diff(points, axis = 1)\n",
    "    rect[1] = points[np.argmin(diff)]\n",
    "    rect[3] = points[np.argmax(diff)]\n",
    "    \"\"\"\n",
    "    maxWidth, maxHeight = get_new_w_h(rect)\n",
    "    dst = np.array([[0, 0],\n",
    "                    [maxWidth - 1, 0],\n",
    "                    [maxWidth - 1, maxHeight - 1],\n",
    "                    [0, maxHeight - 1]],\n",
    "                    dtype = \"float32\")\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warp = cv2.warpPerspective(img.copy(), M, (maxWidth, maxHeight))\n",
    "    return warp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_image_processing(img, kernel):\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # grayscale\n",
    "    lp_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # erosion\n",
    "    lp_gray = cv2.erode(lp_gray, kernel, iterations=1)\n",
    "    # bilateral filter\n",
    "    lp_gray = cv2.bilateralFilter(lp_gray, 5, 75, 75)\n",
    "    # binary thresholding\n",
    "    _, lp_threshold = cv2.threshold(lp_gray, 6, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    return lp_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_license_plate(license_plate, name, save_img=True):\n",
    "    \n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    lp_threshold = apply_image_processing(license_plate, kernel)\n",
    "    \n",
    "    contours, _ = cv2.findContours(lp_threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours:\n",
    "        return None\n",
    "    \n",
    "    # contour with largest area\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    #warped_img = warp_image(license_plate.copy(), max_contour)\n",
    "    \n",
    "    # Crop the license plate region\n",
    "    x, y, w, h = cv2.boundingRect(max_contour)\n",
    "    license_plate_cropped = lp_threshold[y:y + h, x:x + w]\n",
    "\n",
    "    if save_img:\n",
    "        cv2.imwrite(f'thresholded_license_plates/thresh_bil_rectified_{name}', license_plate_cropped)\n",
    "\n",
    "    return license_plate_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read characters on licenese plate using OCR  \n",
    "def read_license_plates(license_plates,name, save_img=True):\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    for lp in license_plates:\n",
    "\n",
    "        #pre processing of the image:\n",
    "        \n",
    "        #1.    grayscale the image\n",
    "       \n",
    "        lp_gray = cv2.cvtColor(lp, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        #2.     erode the image\n",
    "        lp_gray = cv2.erode(lp_gray, kernel, iterations=1)\n",
    "\n",
    "        #3a.     remove noise with median filter\n",
    "        lp_median = cv2.medianBlur(lp_gray,5)\n",
    "        \n",
    "        #3b.     remove noise with bilateral filter\n",
    "        lp_bilateral = cv2.bilateralFilter(lp_gray, 5, 75, 75)\n",
    "\n",
    "        #4a.    canny edge detection \n",
    "        lp_canny = cv2.Canny(lp_median, 100, 200)\n",
    "\n",
    "        #4b.     binary thresholding \n",
    "        _, lp_threshold = cv2.threshold(lp_median, 6, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        _, lp_threshold2 = cv2.threshold(lp_bilateral, 6, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        if save_img:\n",
    "            cv2.imwrite(f'thresholded_license_plates/thresh_median_{name}', lp_threshold)\n",
    "            cv2.imwrite(f'thresholded_license_plates/thresh_bil_{name}', lp_threshold2)\n",
    "            cv2.imwrite(f'thresholded_license_plates/canny_{name}', lp_canny)\n",
    "\n",
    "        return lp_threshold, lp_threshold2, lp_canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "    result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_skew(src_img):\n",
    "\n",
    "    if len(src_img.shape) == 3:\n",
    "        h, w, _ = src_img.shape\n",
    "    elif len(src_img.shape) == 2:\n",
    "        h, w = src_img.shape\n",
    "    else:\n",
    "        print('upsupported image type')\n",
    "\n",
    "    img = cv2.medianBlur(src_img, 3)\n",
    "\n",
    "    edges = cv2.Canny(img,  threshold1 = 30,  threshold2 = 100, apertureSize = 3, L2gradient = True)\n",
    "    lines = cv2.HoughLinesP(edges, 1, math.pi/180, 30, minLineLength=w / 4.0, maxLineGap=h/4.0)\n",
    "    angle = 0.0\n",
    "    nlines = lines.size\n",
    "\n",
    "    #print(nlines)\n",
    "    cnt = 0\n",
    "    for x1, y1, x2, y2 in lines[0]:\n",
    "        ang = np.arctan2(y2 - y1, x2 - x1)\n",
    "        #print(ang)\n",
    "        if math.fabs(ang) <= 30: # excluding extreme rotations\n",
    "            angle += ang\n",
    "            cnt += 1\n",
    "\n",
    "    if cnt == 0:\n",
    "        return 0.0\n",
    "    return (angle / cnt)*180/math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test images 5.8/TÜAR2499/sharp\\image_1.jpg\n",
      "Processing image_1.jpg\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Image Not Found e:\\Users\\belau\\Documents\\BV-ML-CV-Praktikum\\test images 5.8\\TÜAR2499\\sharp\\image_1.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder, img)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(img_path)\n\u001b[1;32m---> 30\u001b[0m detected_licenseplates \u001b[39m=\u001b[39m crop_license_plates(img_path, lp_detect_model)\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m detected_licenseplates:\n\u001b[0;32m     32\u001b[0m     \u001b[39mprint\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mNo license plate detected!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36mcrop_license_plates\u001b[1;34m(img, yolo_model, save_img)\u001b[0m\n\u001b[0;32m      3\u001b[0m img_name \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing \u001b[39m\u001b[39m{\u001b[39;00mimg_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m detected_lps \u001b[39m=\u001b[39m yolo_model\u001b[39m.\u001b[39;49mpredict(img, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m license_plates \u001b[39m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m lp \u001b[39min\u001b[39;00m detected_lps\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtolist():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:255\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m overrides \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m overrides:\n\u001b[0;32m    254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39msave_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mget_save_dir()\n\u001b[1;32m--> 255\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:188\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:231\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch, profilers \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, [], \u001b[39mNone\u001b[39;00m, (ops\u001b[39m.\u001b[39mProfile(), ops\u001b[39m.\u001b[39mProfile(), ops\u001b[39m.\u001b[39mProfile())\n\u001b[0;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks(\u001b[39m'\u001b[39m\u001b[39mon_predict_start\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset:\n\u001b[0;32m    232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks(\u001b[39m'\u001b[39m\u001b[39mon_predict_batch_start\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    233\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m batch\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\ultralytics\\yolo\\data\\dataloaders\\stream_loaders.py:224\u001b[0m, in \u001b[0;36mLoadImages.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m     im0 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(path)  \u001b[39m# BGR\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[39mif\u001b[39;00m im0 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImage Not Found \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimage \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m [path], [im0], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcap, s\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Image Not Found e:\\Users\\belau\\Documents\\BV-ML-CV-Praktikum\\test images 5.8\\TÜAR2499\\sharp\\image_1.jpg"
     ]
    }
   ],
   "source": [
    "def pls_work(license_plate, name):\n",
    "    img_g = cv2.cvtColor(license_plate, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.bilateralFilter(img_g, 11, 17, 17)\n",
    "    img = cv2.threshold(img, 6, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    keypoints = cv2.findContours(img.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = imutils.grab_contours(keypoints)\n",
    "    max_c = max(contours, key=cv2.contourArea)\n",
    "    max_c = cv2.approxPolyDP(max_c, 20, True)\n",
    "    mask = np.zeros(img_g.shape, np.uint8)\n",
    "    new_image = cv2.drawContours(mask, [max_c], 0,255, -1)\n",
    "\n",
    "    new_image = np.where(mask == 255, img_g, 0)\n",
    "    #rotated_img = warp_image(img, max_c)\n",
    "    #TODO Sunday: maybe use minarearectangle to descew\n",
    "    masked_img_gray = cv2.bilateralFilter(new_image, 3,32,32)\n",
    "    masked_img_gray = cv2.threshold(masked_img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    #masked_img_gray = cv2.Canny(masked_img_gray, 30, 200)\n",
    "    cv2.imwrite(\"test_results/masked_\"+name, new_image)\n",
    "    cv2.imwrite(\"test_results/processed_\"+name, masked_img_gray)\n",
    "\n",
    "    return img\n",
    "\n",
    "folder = \"test images 5.8/SHGLF206/sharp\"\n",
    "pattern = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ÜÖ'\n",
    "for img in os.listdir(folder):\n",
    "    if img.endswith('.JPG') or img.endswith('.jpg'):\n",
    "        img_path = os.path.join(folder, img)\n",
    "        print(img_path)\n",
    "        detected_licenseplates = crop_license_plates(img_path, lp_detect_model)\n",
    "        if not detected_licenseplates:\n",
    "            print( \"No license plate detected!\")\n",
    "        else:\n",
    "            test = pls_work(detected_licenseplates[0],img)\n",
    "\n",
    "            res = reader.readtext(test, allowlist= pattern,detail=0,paragraph=True)\n",
    "\n",
    "            print(res)\n",
    "        #print(ocr(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_license_plate_id(img_file):\n",
    "    print(img_file)\n",
    "    detected_licenseplates = crop_license_plates(img_file, lp_detect_model)\n",
    "    assert len(detected_licenseplates) == 1, \"WARNING! More than one License plate detected. Access denied!\"\n",
    "    re_cropped = preprocess_license_plate(detected_licenseplates[0], img_file, save_img=False)\n",
    "    test = pls_work(detected_licenseplates[0])\n",
    "    \n",
    "    res = reader.readtext(test, detail=0, paragraph=True)\n",
    "    print(res)\n",
    "    print(ocr(test))\n",
    "    if re_cropped is not None:\n",
    "        return ocr(re_cropped)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    path_to_cam_frames = 'PythonScripts/cam_frames'\n",
    "    all_lp_ids = []\n",
    "    img_files = [os.path.join(path_to_cam_frames, f) for f in os.listdir(path_to_cam_frames)\n",
    "                 if f.endswith('.jpg') or f.endswith('.png') or f.endswith('.JPG')]\n",
    "    all_lp_ids = map(find_license_plate_id, img_files)\n",
    "    all_lp_ids = [lp_id for lp_id in all_lp_ids if lp_id is not None]\n",
    "    print(all_lp_ids)\n",
    "    predicted_license_plate_id = max(set(all_lp_ids), key=all_lp_ids.count)\n",
    "    print(predicted_license_plate_id)\n",
    "#main()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
